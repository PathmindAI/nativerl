config:
  # Fixed settings (don't touch)
  env_name: factory
  actions: 5
  low: -1
  high: 11

  # Factory settings
  layout: big  # either "big", "medium", or "small"
  scenario: "fixed_6"  # either random, random_fixed_targets, fixed_2, fixed_4, fixed_6, fixed_8, fixed_10, or fixed_12
  num_tables: 6
  num_cores: 6
  num_phases: 1
  with_rails: true  # If "false", remove all rails from the grid
  random_init: true  # if random initialization is False, the factory will reset to the same state after each episode.
  # seed: 1337  # Optional random seed to have full control over initial factory state.

  # RL-specific configuration
  env: "TupleFactoryEnv" # "FactoryEnv", "RoundRobinFactoryEnv", "MultiAgentFactoryEnv" and "TupleFactoryEnv"
  max_num_steps: 10000 # NOTE: need to set this high enough for the big factory
  masking: false  # whether to use action masking TODO: does not work with TupleFactoryEnv (ray issue?)
  autoregressive: false # whether to use autoregressive model TODO: only works with 6-tuple
  algorithm: PPO  # Choose from PPO, DQN, MARWIL
  use_offline_data: false  # Use previously generated offline data (don't use for now, experimental)
  offline_data_ratio: 0.5
  num_samples: 4  # Ray rllib's "num_samples" extracted for convenience
  multi_policy: false  # Using multiple policies or not. This only works for "MultiAgentEnv"
  fcnet_hiddens: [256, 256, 128, 128, 64]  # [512, 512]
  use_lstm: false

  # Observation selection
  ## Agent & core obs
  obs_agent_id: false
  obs_agent_coordinates: false
  obs_agent_has_core: false
  obs_agent_core_target_coordinates: false
  obs_all_table_coordinates: false 

  ## Neighbour obs (unnecessary with action masking)
  obs_agent_has_neighbour: false
  obs_agent_free_neighbour: false

  ## One-hot representation obs: current id and target, plus all tables, cores and targets
  obs_agent_id_one_hot: false 
  obs_agent_core_target_one_hot: false 

  obs_all_tables_one_hot: false 
  obs_all_cores_one_hot: false
  obs_all_targets_one_hot: false

  ## For using round-robin with tuple observations
  obs_agent_table_id_one_hot: false

  ## Overall layout observations 
  obs_all_node_target_pairs_one_hot: false

  ## Tuple observations
  obs_all_table_node_pairs_one_hot: true 
  obs_all_table_target_pairs_one_hot: true 

  ## Reward selection

  ## Positive rewards
  rew_found_target:
    value: false
    weight: 100
  rew_found_target_squared:
    value: false
    weight: 10

  rew_found_target_physical:
    value: true
    weight: 100
  rew_found_target_physical_squared:
    value: false
    weight: 10

  ## Negative rewards
  rew_collisions:
    # it seems for multi-agent there isn't much effect here anyways, masking works well already.
    # collisions can be avoided altogether with tuple + auto-regression, making this term obsolete.
    value: true 
    weight: .1

  # Those two will *always* happen in a "crowded" factory, no matter how smart the algorithm.
  # If these terms are useful, we should set the positive rewards high enough to counteract this.
  rew_blocking_path:
    value: false
    weight: 1
  rew_blocking_target:
    value: false
    weight: 5

  rew_avoid_cores: # does not seem very useful in the current formulation, superseded by "rew_blocking_path"
    value: false
    weight: 1
  rew_punish_slow_tables:
    value: true
    weight: 300
  tighten_max_steps: false # Set to "false" if you don't want to allow less and less steps per episode
  discount_episodes_by: 400
  discount_episodes_until: 0.25
