import glob, gym, nativerl, ray, sys, os, random
import numpy as np
from ray.rllib.env import MultiAgentEnv
from ray.rllib.agents.registry import get_agent_class
from ray.rllib.utils import seed
from ray.tune import run, sample_from
from ray.tune.schedulers import PopulationBasedTraining

from typing import Dict
from ray.rllib.env import BaseEnv
from ray.rllib.policy import Policy
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker
from ray.rllib.agents.callbacks import DefaultCallbacks

jardir = os.getcwd()

class {{classSimpleName environment}}({{#if multiAgent}}MultiAgentEnv{{else}}gym.Env{{/if}}):
    def __init__(self, env_config):
        # Put all JAR files found here in the class path
        jars = glob.glob(jardir + '/**/*.jar', recursive=True)
        nativerl.init(['-Djava.class.path=' + os.pathsep.join(jars + [jardir])]);

        self.nativeEnv = nativerl.createEnvironment('{{className environment}}')
        actionSpace = self.nativeEnv.getActionSpace()
        observationSpace = self.nativeEnv.getObservationSpace()
        self.action_space = gym.spaces.Discrete(actionSpace.n)
        self.observation_space = gym.spaces.Box(observationSpace.low[0], observationSpace.high[0],
                                                np.array(observationSpace.shape), dtype=np.float32)
        self.id = '{{classSimpleName environment}}'
        self.max_episode_steps = 200000
        {{#unless multiAgent}}
        self.unwrapped.spec = self
        {{/unless}}

    def reset(self):
        self.nativeEnv.reset()
        {{#if multiAgent}}
        obs = np.array(self.nativeEnv.getObservation())
        obsdict = {}
        for i in range(0, obs.shape[0]):
            obsdict[str(i)] = obs[i]
        return obsdict
        {{else}}
        return np.array(self.nativeEnv.getObservation())
        {{/if}}

    def step(self, action):
        {{#if multiAgent}}
        actionarray = np.ndarray(shape=(len(action), 1), dtype=np.float32)
        for i in range(0, actionarray.shape[0]):
            actionarray[i, :] = action[str(i)].astype(np.float32)
            reward = np.array(self.nativeEnv.step(nativerl.Array(actionarray)))
            obs = np.array(self.nativeEnv.getObservation())
            obsdict = {}
            rewarddict = {}
            for i in range(0, obs.shape[0]):
                obsdict[str(i)] = obs[i]
                rewarddict[str(i)] = reward[i]
        return obsdict, rewarddict, {'__all__': self.nativeEnv.isDone()}, {}
        {{else}}
        reward = self.nativeEnv.step(action)
        return np.array(self.nativeEnv.getObservation()), reward, self.nativeEnv.isDone(), {}
        {{/if}}

    def getMetrics(self):
        return np.array(self.nativeEnv.getMetrics())

      
class Callbacks(DefaultCallbacks):
   def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,
                      policies: Dict[str, Policy], episode: MultiAgentEpisode, **kwargs):
       print("kepricondebug on_episode_end : {} ".format(worker.env.getMetrics()))


   def on_train_result(self, trainer, result: dict, **kwargs):
       #print("trainer.train() result: {} -> {} episodes".format(trainer, result["episodes_this_iter"]))

       from ray.rllib.utils.memory import ray_get_and_free
       remote_results = ray_get_and_free(
           [w.apply.remote(lambda worker: worker.env.getMetrics()) for w in trainer.workers.remote_workers()])
       print("kepricondebug on_train_result : {}".format(remote_results))

       # you can mutate the result dict to add new fields to return
       result["metrics"] = remote_results


class Stopper:
    def __init__(self):
        # Core criteria
        self.should_stop = False # Stop criteria met
        self.too_many_iter = False # Max iterations
        self.too_much_time = False # Max training time
        self.too_many_episodes = False # Max total episodes

        # Stopping criteria at early check
        self.no_discovery_risk = False # Value loss never changes
        self.no_converge_risk = False # Entropy never drops

        # Convergence signals at each iteration from converge check onward
        self.episode_reward_converged = False # Reward mean changes very little
        self.value_pred_converged = False # Explained variance changes very little

        # Episode reward behaviour
        self.episode_reward_window = []
        self.episode_reward_range = 0
        self.episode_reward_mean = 0
        self.episode_reward_mean_latest = 0

        # Entropy behaviour
        self.entropy_start = 0
        self.entropy_now = 0
        self.entropy_slope = 0

        # Value loss behaviour
        self.vf_loss_window = []
        self.vf_loss_range = 0
        self.vf_pred_window = []
        self.vf_pred_mean = 0
        self.vf_pred_mean_latest = 0

        # Configs
        self.episode_reward_range_threshold = {{episodeRewardRangeTh}}  # Remove with 0
        self.entropy_slope_threshold = {{entropySlopeTh}}  # Remove with 1
        self.vf_loss_range_threshold = {{vfLossRangeTh}}  # Remove with 0
        self.value_pred_threshold = {{valuePredTh}}  # Remove with 0

    def stop(self, trial_id, result):
        # Core Criteria
        self.too_many_iter = result['training_iteration'] >= {{maxIterations}}
        {{#if (gt maxTimeInSec 0)}}
        self.too_much_time = result['time_total_s'] >= {{maxTimeInSec}}
        {{/if}}
        self.too_many_episodes = result['episodes_total'] >= 30000

        if not self.should_stop and (self.too_many_iter or self.too_much_time or self.too_many_episodes):
            self.should_stop = True
            return self.should_stop

        # Collecting metrics for stopping criteria
        if result['training_iteration'] == 1:
            #self.entropy_start = result['info/learner/default_policy/entropy']
            self.entropy_start = result['info']['learner']['default_policy']['entropy']

        if result['training_iteration'] <= 50:
            #self.vf_loss_window.append(result['info/learner/default_policy/vf_loss'])
            self.vf_loss_window.append(result['info']['learner']['default_policy']['vf_loss'])

        self.episode_reward_window.append(result['episode_reward_mean'])
        #self.vf_pred_window.append(result['info/learner/default_policy/vf_explained_var'])
        self.vf_pred_window.append(result['info']['learner']['default_policy']['vf_explained_var'])

        # Experimental Criteria

        # Early stopping filter
        if result['training_iteration'] == 50:
            #self.entropy_now = result['info/learner/default_policy/entropy']
            self.entropy_now = result['info']['learner']['default_policy']['entropy']
            self.entropy_slope = self.entropy_now - self.entropy_start
            self.vf_loss_range = np.max(np.array(self.vf_loss_window)) - np.min(np.array(self.vf_loss_window))
            if self.entropy_slope > np.abs(self.entropy_start * self.entropy_slope_threshold):
                self.no_converge_risk = True
            if np.abs(self.vf_loss_range) < np.abs(self.vf_loss_window[0] * self.vf_loss_range_threshold):
                self.no_discovery_risk = True

            # Early stopping decision
            if not self.should_stop and (self.no_converge_risk or self.no_discovery_risk):
                self.should_stop = True
                return self.should_stop

        # Convergence Filter
        if result['training_iteration'] >= 125:
            # Episode reward range activity
            self.episode_reward_range = \
                np.max(np.array(self.episode_reward_window[-50:])) - np.min(np.array(self.episode_reward_window[-50:]))
            # Episode reward mean activity
            self.episode_reward_mean = np.mean(np.array(self.episode_reward_window[-75:]))
            self.episode_reward_mean_latest = np.mean(np.array(self.episode_reward_window[-15:]))
            # Value function activity
            self.vf_pred_mean = np.mean(np.array(self.vf_pred_window[-25:]))
            self.vf_pred_mean_latest = np.mean(np.array(self.vf_pred_window[-5:]))

            # Episode reward leveled off
            if (np.abs(self.episode_reward_mean_latest - self.episode_reward_mean) / np.abs(self.episode_reward_mean)
                < self.episode_reward_range_threshold) and \
                    (np.abs(self.episode_reward_range) <
                     np.abs(np.mean(np.array(self.episode_reward_window[-50:])) * 2)):
                self.episode_reward_converged = True

            # Explained variance leveled off
            if (np.abs(self.vf_pred_mean_latest - self.vf_pred_mean) / np.abs(self.vf_pred_mean)
                    < self.value_pred_threshold):
                self.value_pred_converged = True

            # Convergence stopping decision
            if not self.should_stop and self.episode_reward_converged and self.value_pred_converged:
                self.should_stop = True
                return self.should_stop

        # Returns False by default until stopping decision made
        return self.should_stop

stopper = Stopper()

pbt_scheduler = PopulationBasedTraining(
    time_attr = 'training_iteration',
    metric = 'episode_reward_mean',
    mode = 'max',
    perturbation_interval = 10,
    quantile_fraction = 0.25,
    resample_probability = 0.25,
    log_config = True,
    hyperparam_mutations = {
        'lambda': np.linspace(0.9, 1.0, 5).tolist(),
        'clip_param': np.linspace(0.01, 0.5, 5).tolist(),
        'entropy_coeff': np.linspace(0, 0.03, 5).tolist(),
        'lr': [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],
        'num_sgd_iter': [5, 10, 15, 20, 30],
        'sgd_minibatch_size': [128, 256, 512, 1024, 2048],
        'train_batch_size': [4000, 6000, 8000, 10000, 12000]
    }
)

# Make sure multiple processes can read the database from AnyLogic
with open('database/db.properties', 'r+') as f:
    lines = f.readlines()
    if 'hsqldb.lock_file=false\n' not in lines:
        f.write('hsqldb.lock_file=false\n')

ray.init(log_to_driver={{#if userLog}}True{{else}}False{{/if}})
model = ray.rllib.models.MODEL_DEFAULTS.copy()
model['fcnet_hiddens'] = {{hiddenLayers}}

trials = run(
    'PPO',
    scheduler=pbt_scheduler,
    num_samples={{numSamples}},
    stop = stopper.stop,
    config = {
        'env': {{classSimpleName environment}},
        'callbacks': Callbacks,
        'num_gpus': 0,
        'num_workers': {{numWorkers}} ,
        'num_cpus_per_worker': {{numCPUs}},
        'model': model,
        'use_gae': True,
        'vf_loss_coeff': 1.0,
        'vf_clip_param': np.inf,
        # These params are tuned from a fixed starting value.
        'lambda': 0.95,
        'clip_param': 0.2,
        'lr': 1e-4,
        'entropy_coeff': 0.0,
        # These params start off randomly drawn from a set.
        'num_sgd_iter': sample_from(
                lambda spec: random.choice([10, 20, 30])),
        'sgd_minibatch_size': sample_from(
                lambda spec: random.choice([128, 512, 2048])),
        'train_batch_size': sample_from(
                lambda spec: random.choice([4000, 8000, 12000])),
        # Set rollout samples to episode length
        'batch_mode': 'complete_episodes',
        # Auto-normalize observations
        #'observation_filter': 'MeanStdFilter'
    },
    {{#if outputDir}}local_dir='{{outputDir.absolutePath}}{{else}}""{{/if}}',
    resume = {{#if resume}}True{{else}}False{{/if}},
    checkpoint_freq = {{checkpointFrequency}} ,
    checkpoint_at_end = True,
    max_failures = 1,
    export_formats = ['model'],
    return_trials = True
)
errored_trials = []
for trial in trials:
    if trial.status != 'TERMINATED':
        errored_trials += [trial]

if errored_trials:
    print(errored_trials)
else:
    print("Training has been completed");