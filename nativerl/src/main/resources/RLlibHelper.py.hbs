import glob, gym, nativerl, ray, sys, os, random, numpy
from ray.rllib.agents.registry import get_agent_class
from ray.rllib.utils import seed
from ray.tune import run, sample_from
from ray.tune.schedulers import PopulationBasedTraining
from gym.spaces import Box, Discrete, Tuple
{{~#if autoregressive}}
from ray.rllib.models import ModelCatalog
from ray.rllib.models.tf.tf_action_dist import Categorical, ActionDistribution
from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.utils.tuple_actions import TupleActions
from ray.rllib.models.tf.misc import normc_initializer
from ray.rllib.utils import try_import_tf

tf = try_import_tf()
{{/if}}
jardir = os.getcwd()

class {{classSimpleName environment}}(gym.Env):
    def __init__(self, env_config):
        # Put all JAR files found here in the class path
        jars = glob.glob(jardir + '/**/*.jar', recursive=True)
        nativerl.init(['-Djava.class.path=' + os.pathsep.join(jars + [jardir])]);

        self.nativeEnv = nativerl.createEnvironment('{{className environment}}')
        actionSpace = self.nativeEnv.getActionSpace()
        observationSpace = self.nativeEnv.getObservationSpace()
        {{#if (eq actionTupleSize 1)~}}
        self.action_space = gym.spaces.Discrete(actionSpace.n)
        {{~else~}}
        self.action_space = Tuple([gym.spaces.Discrete(actionSpace.n) for i in range({{actionTupleSize}})])
        {{~/if}}
        self.observation_space = gym.spaces.Box(observationSpace.low[0], observationSpace.high[0], numpy.array(observationSpace.shape), dtype=numpy.float32)
        self.id = '{{classSimpleName environment}}'
        self.max_episode_steps = 20000
        self.unwrapped.spec = self

    def reset(self):
        self.nativeEnv.reset()
        return numpy.array(self.nativeEnv.getObservation())

    def step(self, action):
        {{~#if (eq actionTupleSize 1)}}
        actionarray = numpy.ndarray(shape=(1, 1), dtype=numpy.float32)
        {{~else}}
        actionarray = numpy.ndarray(shape=(1, len(action)), dtype=numpy.float32)
        {{~/if}}
        for i in range(0, actionarray.shape[1]):
            {{~#if (eq actionTupleSize 1)}}
            actionarray[0,i] = action
            {{~else}}
            actionarray[0,i] = action[i].astype(numpy.float32)
            {{~/if}}
        reward = self.nativeEnv.step(nativerl.Array(actionarray))
        return numpy.array(self.nativeEnv.getObservation()), reward, self.nativeEnv.isDone(), {}

class Stopper:
    def __init__(self):
        # Core criteria
        self.too_many_iter = False # Max iterations
        self.too_much_time = False # Max training time
        self.too_many_episodes = False # Max total episodes

        # Stopping criteria at early check
        self.no_discovery_risk = False # Value loss never changes
        self.no_converge_risk = False # Entropy never drops

        # Convergence signals at each iteration from converge check onward
        self.episode_reward_converged = False # Reward mean changes very little
        self.value_pred_converged = False # Explained variance changes very little

        # Episode reward behaviour
        self.episode_reward_window = {}
        self.episode_reward_range = 0
        self.episode_reward_mean = 0
        self.episode_reward_mean_latest = 0

        # Entropy behaviour
        self.entropy_start = 0
        self.entropy_now = 0
        self.entropy_slope = 0

        # Value loss behaviour
        self.vf_loss_window = []
        self.vf_loss_range = 0
        self.vf_pred_window = []
        self.vf_pred_mean = 0
        self.vf_pred_mean_latest = 0

        # Configs
        self.episode_reward_range_threshold = {{episodeRewardRangeTh}}  # Remove with 0
        self.entropy_slope_threshold = {{entropySlopeTh}}  # Remove with 1
        self.vf_loss_range_threshold = {{vfLossRangeTh}}  # Remove with 0
        self.value_pred_threshold = {{valuePredTh}}  # Remove with 0

    def stop(self, trial_id, result):
        # Core Criteria
        self.too_many_iter = result['training_iteration'] >= {{maxIterations}}
        {{#if (gt maxTimeInSec 0)~}}
        self.too_much_time = result['time_total_s'] >= {{maxTimeInSec}}
        {{/if~}}
        self.too_many_episodes = result['episodes_total'] >= 30000

        # Stop entire experiment if max training ceiling reached
        if self.too_many_iter or self.too_much_time or self.too_many_episodes:
            return True

        # Collecting metrics for stopping criteria
        if result['training_iteration'] == 1:
            self.entropy_start = result['info']['learner']['default_policy']['entropy']

        if result['training_iteration'] <= 50:
            self.vf_loss_window.append(result['info']['learner']['default_policy']['vf_loss'])

        if (not trial_id in self.episode_reward_window):
            self.episode_reward_window[trial_id] = []
        self.episode_reward_window[trial_id].append(result['episode_reward_mean'])
        self.vf_pred_window.append(result['info']['learner']['default_policy']['vf_explained_var'])

        # Early stopping filter
        if result['training_iteration'] == 50:
            self.entropy_now = result['info']['learner']['default_policy']['entropy']
            self.entropy_slope = self.entropy_now - self.entropy_start
            self.vf_loss_range = numpy.max(numpy.array(self.vf_loss_window)) - numpy.min(numpy.array(self.vf_loss_window))

            if self.entropy_slope > numpy.abs(self.entropy_start * self.entropy_slope_threshold):
                self.no_converge_risk = True
            if numpy.abs(self.vf_loss_range) < numpy.abs(self.vf_loss_window[0] * self.vf_loss_range_threshold):
                self.no_discovery_risk = True

            # Stop entire experiment if no learning occurs
            if self.no_converge_risk or self.no_discovery_risk:
                return True

        # Convergence Filter
        if result['training_iteration'] >= 150:
            # Episode reward range activity
            self.episode_reward_range = numpy.max(numpy.array(self.episode_reward_window[-50:])) - numpy.min(numpy.array(self.episode_reward_window[-50:]))
            # Episode reward mean activity
            self.episode_reward_mean = numpy.mean(numpy.array(self.episode_reward_window[-75:]))
            self.episode_reward_mean_latest = numpy.mean(numpy.array(self.episode_reward_window[-15:]))
            # Value function activity
            self.vf_pred_mean = numpy.mean(numpy.array(self.vf_pred_window[-25:]))
            self.vf_pred_mean_latest = numpy.mean(numpy.array(self.vf_pred_window[-5:]))

            # Episode reward leveled off
            if (numpy.abs(self.episode_reward_mean_latest - self.episode_reward_mean) / numpy.abs(self.episode_reward_mean) < self.episode_reward_range_threshold) and (numpy.abs(self.episode_reward_range) < numpy.abs(numpy.mean(numpy.array(self.episode_reward_window[trial_id][-50:])) * 2)):
                self.episode_reward_converged = True

            # Explained variance leveled off
            if (numpy.abs(self.vf_pred_mean_latest - self.vf_pred_mean) / numpy.abs(self.vf_pred_mean) < self.value_pred_threshold):
                self.value_pred_converged = True

            # Stop individual trial when convergence criteria met
            if self.episode_reward_converged and self.value_pred_converged:
                return trial_id

stopper = Stopper()

pbt_scheduler = PopulationBasedTraining(
    time_attr = 'training_iteration',
    metric = 'episode_reward_mean',
    mode = 'max',
    perturbation_interval = 10,
    quantile_fraction = 0.25,
    resample_probability = 0.25,
    log_config = True,
    hyperparam_mutations = {
        'lambda': numpy.linspace(0.9, 1.0, 5).tolist(),
        'clip_param': numpy.linspace(0.01, 0.5, 5).tolist(),
        'entropy_coeff': numpy.linspace(0, 0.03, 5).tolist(),
        'lr': [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],
        'num_sgd_iter': [5, 10, 15, 20, 30],
        'sgd_minibatch_size': [128, 256, 512, 1024, 2048],
        'train_batch_size': [4000, 6000, 8000, 10000, 12000]
    }
)

# Make sure multiple processes can read the database from AnyLogic
with open('database/db.properties', 'r+') as f:
    lines = f.readlines()
    if 'hsqldb.lock_file=false\n' not in lines:
        f.write('hsqldb.lock_file=false\n')

ray.init(log_to_driver={{#if userLog}}True{{else}}False{{/if}}, webui_host='127.0.0.1', lru_evict=True)
{{#if autoregressive~}}
{{{autoregressiveModel}}}
{{~else~}}
model = ray.rllib.models.MODEL_DEFAULTS.copy()
model['fcnet_hiddens'] = {{hiddenLayers}}
{{~/if}}

trials = run(
    'PPO',
    scheduler = pbt_scheduler,
    num_samples = {{numSamples}},
    stop = Stopper().stop,
    config = {
        'env': {{classSimpleName environment}},
        'num_gpus': 0,
        'num_workers': {{numWorkers}} ,
        'num_cpus_per_worker': {{numCPUs}},
        {{#if autoregressive~}}
        'model': {
            'custom_model': 'autoregressive_model',
            'custom_action_dist': 'nary_autoreg_output',
        },
        {{~else~}}
        'model': model, {{/if}}
        'use_gae': True,
        'vf_loss_coeff': 1.0,
        'vf_clip_param': numpy.inf,
        # These params are tuned from a fixed starting value.
        'lambda': 0.95,
        'clip_param': 0.2,
        'lr': 1e-4,
        'entropy_coeff': 0.0,
        # These params start off randomly drawn from a set.
        'num_sgd_iter': sample_from(
                lambda spec: random.choice([10, 20, 30])),
        'sgd_minibatch_size': sample_from(
                lambda spec: random.choice([128, 512, 2048])),
        'train_batch_size': sample_from(
                lambda spec: random.choice([4000, 8000, 12000])),
        # Set rollout samples to episode length
        'batch_mode': 'complete_episodes',
        # Auto-normalize observations
        #'observation_filter': 'MeanStdFilter'
    },
    {{#if outputDir}}local_dir = '{{outputDir.absolutePath}}{{else}}""{{/if}}',
    resume = {{#if resume}}True{{else}}False{{/if}},
    checkpoint_freq = {{checkpointFrequency}} ,
    checkpoint_at_end = True,
    max_failures = 1,
    export_formats = ['model'],
    return_trials = True
)

errored_trials = []
for trial in trials:
    if trial.status != 'TERMINATED':
        errored_trials += [trial]

if errored_trials:
    print(errored_trials)
else:
    print("Training has been completed");
