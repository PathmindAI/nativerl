import glob, gym, nativerl, ray, sys, os, random, numpy
from ray.rllib.env import MultiAgentEnv
from ray.rllib.agents.registry import get_agent_class
from ray.tune import run, sample_from
from ray.tune.schedulers import PopulationBasedTraining
from gym.spaces import Box, Discrete, Tuple
from typing import Dict
from ray.rllib.env import BaseEnv
from ray.rllib.policy import Policy
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker
from ray.rllib.agents.callbacks import DefaultCallbacks
from ray.tune.logger import CSVLogger
from ray.tune.logger import DEFAULT_LOGGERS
from ray.tune.result import EXPR_PROGRESS_FILE
{{~#if autoregressive}}
from ray.rllib.models import ModelCatalog
from ray.rllib.models.tf.tf_action_dist import Categorical, ActionDistribution
from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.utils.tuple_actions import TupleActions
from ray.rllib.models.tf.misc import normc_initializer
from ray.rllib.utils import try_import_tf

tf = try_import_tf()
{{/if}}
jardir = os.getcwd()

class {{classSimpleName environment}}({{#if multiAgent}}MultiAgentEnv{{else}}gym.Env{{/if}}):
    def __init__(self, env_config):
        # AnyLogic needs this to find its database
        os.chdir(jardir);

        # Put all JAR files found here in the class path
        jars = glob.glob(jardir + '/**/*.jar', recursive=True)
        nativerl.init(['-Djava.class.path=' + os.pathsep.join(jars + [jardir]), '-Xmx{{maxMemoryInMB}}m']);

        self.nativeEnv = nativerl.createEnvironment('{{className environment}}')

        i = 0
        actionSpace = self.nativeEnv.getActionSpace(i)
        actionSpaces = []
        while actionSpace is not None:
            if isinstance(actionSpace, nativerl.Discrete):
                actionSpaces += [gym.spaces.Discrete(actionSpace.n) for j in range(actionSpace.size)]
            else:
                actionSpaces += [gym.spaces.Box(0, 1, numpy.array(actionSpace.shape), dtype=numpy.float32)]
            i += 1
            actionSpace = self.nativeEnv.getActionSpace(i)
        self.action_space = actionSpaces[0] if len(actionSpaces) == 1 else gym.spaces.Tuple(actionSpaces)

        observationSpace = self.nativeEnv.getObservationSpace()
        low = observationSpace.low[0] if len(observationSpace.low) == 1 else numpy.array(observationSpace.low)
        high = observationSpace.high[0] if len(observationSpace.high) == 1 else numpy.array(observationSpace.high)
        self.observation_space = gym.spaces.Box(low, high, numpy.array(observationSpace.shape), dtype=numpy.float32)

        actionMaskSpace = self.nativeEnv.getActionMaskSpace()
        if actionMaskSpace is not None:
            low = actionMaskSpace.low[0] if len(actionMaskSpace.low) == 1 else numpy.array(actionMaskSpace.low)
            high = actionMaskSpace.high[0] if len(actionMaskSpace.high) == 1 else numpy.array(actionMaskSpace.high)
            self.observation_space = gym.spaces.Dict({
                "action_mask": gym.spaces.Box(low, high, numpy.array(actionMaskSpace.shape), dtype=numpy.float32),
                "real_obs": self.observation_space
            })

        self.id = '{{classSimpleName environment}}'
        self.max_episode_steps = 20000
{{#if multiAgent}}
{{else}}
        self.unwrapped.spec = self
{{/if}}

    def reset(self):
        self.nativeEnv.reset()
{{#if multiAgent}}
        obsdict = {}
        for i in range(0, self.nativeEnv.getNumberOfAgents()):
            obs = numpy.array(self.nativeEnv.getObservation(i))
            if isinstance(self.observation_space, gym.spaces.Dict):
                obs = {"action_mask": numpy.array(self.nativeEnv.getActionMask(i)), "real_obs": obs}
            obsdict[str(i)] = obs
        return obsdict
{{else}}
        if self.nativeEnv.getNumberOfAgents() != 1:
            raise ValueError("Not in multi-agent mode: Number of agents needs to be 1")
        obs = numpy.array(self.nativeEnv.getObservation())
        if isinstance(self.observation_space, gym.spaces.Dict):
            obs = {"action_mask": numpy.array(self.nativeEnv.getActionMask()), "real_obs": obs}
        return obs
{{/if}}

    def step(self, action):
{{#if multiAgent}}
        for i in range(0, self.nativeEnv.getNumberOfAgents()):
            act = action[str(i)]
            if isinstance(self.action_space, gym.spaces.Tuple):
                actionArray = numpy.empty(shape=(0), dtype=numpy.float32)
                for j in range(0, len(act)):
                    actionArray = numpy.concatenate([actionArray, act[j].astype(numpy.float32)], axis=None)
            else:
                actionArray = act.astype(numpy.float32)
            self.nativeEnv.setNextAction(nativerl.Array(actionArray), i)

        self.nativeEnv.step()

        obsdict = {}
        rewarddict = {}
        donedict = {}
        for i in range(0, self.nativeEnv.getNumberOfAgents()):
            obs = numpy.array(self.nativeEnv.getObservation(i))
            if isinstance(self.observation_space, gym.spaces.Dict):
                obs = {"action_mask": numpy.array(self.nativeEnv.getActionMask(i)), "real_obs": obs}
            obsdict[str(i)] = obs
            rewarddict[str(i)] = self.nativeEnv.getReward(i)
            donedict[str(i)] = self.nativeEnv.isDone(i)
        donedict['__all__'] = self.nativeEnv.isDone(-1)
        return obsdict, rewarddict, donedict, {}
{{else}}
        if self.nativeEnv.getNumberOfAgents() != 1:
            raise ValueError("Not in multi-agent mode: Number of agents needs to be 1")
        if isinstance(self.action_space, gym.spaces.Tuple):
            actionArray = numpy.empty(shape=(0), dtype=numpy.float32)
            for j in range(0, len(action)):
                actionArray = numpy.concatenate([actionArray, action[j].astype(numpy.float32)], axis=None)
        else:
            actionArray = action.astype(numpy.float32)
        self.nativeEnv.setNextAction(nativerl.Array(actionArray))
        self.nativeEnv.step()
        reward = self.nativeEnv.getReward()
        obs = numpy.array(self.nativeEnv.getObservation())
        if isinstance(self.observation_space, gym.spaces.Dict):
            obs = {"action_mask": numpy.array(self.nativeEnv.getActionMask()), "real_obs": obs}
        return obs, reward, self.nativeEnv.isDone(), {}
{{/if}}

    # TODO: This needs to be updated somehow to return metrics for all agents
    def getMetrics(self):
        return numpy.array(self.nativeEnv.getMetrics(0))

class Callbacks(DefaultCallbacks):
    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,
                        policies: Dict[str, Policy],
                        episode: MultiAgentEpisode, **kwargs):
        episode.hist_data["metrics_raw"] = []

    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,
                        policies: Dict[str, Policy], episode: MultiAgentEpisode, **kwargs):
        metrics = worker.env.getMetrics().tolist()

        {{#if debugMetrics}}
        episode.hist_data["metrics_raw"] = metrics
        {{/if}}

        for i, val in enumerate(metrics):
            episode.custom_metrics["metrics_" + str(i)] = metrics[i]

    def on_train_result(self, trainer, result: dict, **kwargs):
        results = ray.get(
            [w.apply.remote(lambda worker: worker.env.getMetrics()) for w in trainer.workers.remote_workers()])

        result["last_metrics"] = results[0].tolist() if results != None and len(results) > 0 else -1


class Stopper:
    def __init__(self):
        # Core criteria
        self.too_many_iter = False # Max iterations
        self.too_much_time = False # Max training time
        self.too_many_episodes = False # Max total episodes

        # Stopping criteria at early check
        self.no_discovery_risk = False # Value loss never changes
        self.no_converge_risk = False # Entropy never drops

        # Convergence signals at each iteration from converge check onward
        self.episode_reward_converged = False # Reward mean changes very little
        self.value_pred_converged = False # Explained variance changes very little

        # Episode reward behaviour
        self.episode_reward_window = {}
        self.episode_reward_range = 0
        self.episode_reward_mean = 0
        self.episode_reward_mean_latest = 0

        # Entropy behaviour
        self.entropy_start = 0
        self.entropy_now = 0
        self.entropy_slope = 0

        # Value loss behaviour
        self.vf_loss_window = []
        self.vf_loss_range = 0
        self.vf_pred_window = []
        self.vf_pred_mean = 0
        self.vf_pred_mean_latest = 0

        # Configs
        self.episode_reward_range_threshold = {{episodeRewardRangeTh}} # Turn off with 0
        self.entropy_slope_threshold = {{entropySlopeTh}} # Turn off with 1
        self.vf_loss_range_threshold = {{vfLossRangeTh}} # Turn off with 0
        self.value_pred_threshold = {{valuePredTh}} # Turn off with 0

    def stop(self, trial_id, result):
        # Core stopping criteria
        self.too_many_iter = result['training_iteration'] >= {{maxIterations}}
        {{#if (gt maxTimeInSec 0)~}}
        self.too_much_time = result['time_total_s'] >= {{maxTimeInSec}}
        {{/if~}}
        self.too_many_episodes = result['episodes_total'] >= 30000

        # Stop entire experiment if max training ceiling reached
        if self.too_many_iter or self.too_much_time or self.too_many_episodes:
            return True

        # Collect metrics for stopping criteria
        if result['training_iteration'] == 1:
            self.entropy_start = result['info']['learner']['default_policy']['entropy']

        if result['training_iteration'] <= 50:
            self.vf_loss_window.append(result['info']['learner']['default_policy']['vf_loss'])

        if (not trial_id in self.episode_reward_window):
            self.episode_reward_window[trial_id] = []
        self.episode_reward_window[trial_id].append(result['episode_reward_mean'])
        self.vf_pred_window.append(result['info']['learner']['default_policy']['vf_explained_var'])

        # Early learning check
        if result['training_iteration'] == 50:
            self.entropy_now = result['info']['learner']['default_policy']['entropy']
            self.entropy_slope = self.entropy_now - self.entropy_start
            self.vf_loss_range = numpy.max(numpy.array(self.vf_loss_window)) - numpy.min(numpy.array(self.vf_loss_window))

            if self.entropy_slope > numpy.abs(self.entropy_start * self.entropy_slope_threshold):
                self.no_converge_risk = True
            if numpy.abs(self.vf_loss_range) < numpy.abs(self.vf_loss_window[0] * self.vf_loss_range_threshold):
                self.no_discovery_risk = True

            # Stop entire experiment if no learning occurs
            if self.no_converge_risk or self.no_discovery_risk:
                return True

        # Convergence check
        if result['training_iteration'] >= 150:
            # Episode reward range activity
            self.episode_reward_range = numpy.max(numpy.array(self.episode_reward_window[trial_id][-50:])) - numpy.min(numpy.array(self.episode_reward_window[trial_id][-50:]))
            # Episode reward mean activity
            self.episode_reward_mean = numpy.mean(numpy.array(self.episode_reward_window[trial_id][-75:]))
            self.episode_reward_mean_latest = numpy.mean(numpy.array(self.episode_reward_window[trial_id][-15:]))
            # Value function activity
            self.vf_pred_mean = numpy.mean(numpy.array(self.vf_pred_window[-25:]))
            self.vf_pred_mean_latest = numpy.mean(numpy.array(self.vf_pred_window[-5:]))

            # Episode reward leveled off
            if (numpy.abs(self.episode_reward_mean_latest - self.episode_reward_mean) / numpy.abs(self.episode_reward_mean) < self.episode_reward_range_threshold) and (numpy.abs(self.episode_reward_range) < numpy.abs(numpy.mean(numpy.array(self.episode_reward_window[trial_id][-50:])) * 2)):
                self.episode_reward_converged = True

            # Explained variance leveled off
            if (numpy.abs(self.vf_pred_mean_latest - self.vf_pred_mean) / numpy.abs(self.vf_pred_mean) < self.value_pred_threshold):
                self.value_pred_converged = True

            # Stop individual trial when convergence criteria met
            if self.episode_reward_converged and self.value_pred_converged:
                return trial_id


class PM_CSVLogger(CSVLogger):
    def _init(self):
        """CSV outputted with Headers as first set of results."""
        progress_file = os.path.join(self.logdir, EXPR_PROGRESS_FILE)
        self._continuing = os.path.exists(progress_file) and os.path.getsize(progress_file) > 0
        self._file = open(progress_file, "a")
        self._csv_out = None

LOGGERS = list(DEFAULT_LOGGERS)
LOGGERS[1] = PM_CSVLogger

pbt_scheduler = PopulationBasedTraining(
    time_attr = 'training_iteration',
    metric = 'episode_reward_mean',
    mode = 'max',
    perturbation_interval = 10,
    quantile_fraction = 0.25,
    resample_probability = 0.25,
    log_config = True,
    hyperparam_mutations = {
        'lambda': numpy.linspace(0.9, 1.0, 5).tolist(),
        'clip_param': numpy.linspace(0.01, 0.5, 5).tolist(),
        'entropy_coeff': numpy.linspace(0, 0.03, 5).tolist(),
        'lr': [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],
        'num_sgd_iter': [5, 10, 15, 20, 30],
        'sgd_minibatch_size': [128, 256, 512, 1024, 2048],
        'train_batch_size': [4000, 6000, 8000, 10000, 12000]
    }
)

# Make sure multiple processes can read the database from AnyLogic
if os.path.isfile('database/db.properties'):
    with open('database/db.properties', 'r+') as f:
        lines = f.readlines()
        if 'hsqldb.lock_file=false\n' not in lines:
            f.write('hsqldb.lock_file=false\n')

ray.init(log_to_driver={{#if userLog}}True{{else}}False{{/if}}, webui_host='127.0.0.1', lru_evict=True)
{{#if autoregressive~}}
{{{autoregressiveModel}}}
{{~else~}}
model = ray.rllib.models.MODEL_DEFAULTS.copy()
model['fcnet_hiddens'] = {{hiddenLayers}}
{{~/if}}

trials = run(
    'PPO',
    scheduler = pbt_scheduler,
    num_samples = {{numSamples}},
    stop = Stopper().stop,
    loggers = LOGGERS,
    config = {
        'env': {{classSimpleName environment}},
        'callbacks': Callbacks,
        'num_gpus': 0,
        'num_workers': {{numWorkers}},
        'num_cpus_per_worker': {{numCPUs}},
        {{#if autoregressive~}}
        'model': {
            'custom_model': 'autoregressive_model',
            'custom_action_dist': 'nary_autoreg_output',
        },
        {{~else~}}
        'model': model,{{/if}}
        'use_gae': True,
        'vf_loss_coeff': 1.0,
        'vf_clip_param': numpy.inf,
        # These params are tuned from a fixed starting value.
        'lambda': 0.95,
        'clip_param': 0.2,
        'lr': 1e-4,
        'entropy_coeff': 0.0,
        # These params start off randomly drawn from a set.
        'num_sgd_iter': sample_from(
                lambda spec: random.choice([10, 20, 30])),
        'sgd_minibatch_size': sample_from(
                lambda spec: random.choice([128, 512, 2048])),
        'train_batch_size': sample_from(
                lambda spec: random.choice([4000, 8000, 12000])),
        # Set rollout samples to episode length
        'batch_mode': 'complete_episodes',
        # Auto-normalize observations
        #'observation_filter': 'MeanStdFilter'
    },
    {{#if outputDir}}local_dir = '{{outputDir.absolutePath}}{{else}}""{{/if}}',
    resume = {{#if resume}}True{{else}}False{{/if}},
    checkpoint_freq = {{checkpointFrequency}},
    checkpoint_at_end = True,
    max_failures = 1,
    export_formats = ['model'],
    return_trials = True
)

errored_trials = []
for trial in trials:
    if trial.status != 'TERMINATED':
        errored_trials += [trial]

if errored_trials:
    print(errored_trials)
else:
    print("Training has been completed")
